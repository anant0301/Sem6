{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROBLEM SET V – FREQUENT PATTERN MINING IMPLEMENTATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.preprocessing import TransactionEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apyori\n",
    "import py_dic\n",
    "import pyfpgrowth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install apyori\n",
    "\n",
    "[Link for apyori](https://pypi.org/project/apyori/)\n",
    "\n",
    "pip install mlxtend\n",
    "\n",
    "[Link for mlxtend](https://pypi.org/project/mlxtend/)\n",
    "\n",
    "pip install pyfpgrowth\n",
    "\n",
    "[Link for pyfpgrowth](https://pypi.org/project/pyfpgrowth/)\n",
    "\n",
    "pip install py-dic\n",
    "\n",
    "[Link for py-dic](https://pypi.org/project/py-dic/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.\n",
    "Test drive the basic version of Apriori and FP Growth algorithms for Frequent Itemset Mining using the package / library support in the platform of your choice. Test it with various support and confidence measures and generate a time comparison for varied data set sizes. To do the performance comparison you may use benchmark datasets provided for FIM such as the FIMI workshop or other sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.genfromtxt('connect.dat', dtype= int, delimiter=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = list(apyori.apriori(data, min_support= 0.98))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[RelationRecord(items=frozenset({19}), support=0.9923472031025652, ordered_statistics=[OrderedStatistic(items_base=frozenset(), items_add=frozenset({19}), confidence=0.9923472031025652, lift=1.0)]),\n",
       " RelationRecord(items=frozenset({37}), support=0.9923768077327294, ordered_statistics=[OrderedStatistic(items_base=frozenset(), items_add=frozenset({37}), confidence=0.9923768077327294, lift=1.0)]),\n",
       " RelationRecord(items=frozenset({55}), support=0.9924064123628935, ordered_statistics=[OrderedStatistic(items_base=frozenset(), items_add=frozenset({55}), confidence=0.9924064123628935, lift=1.0)])]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FP Growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fp on min_sup = 0.98\n",
    "pattern = pyfpgrowth.find_frequent_patterns(data, 0.98)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. \n",
    "Extend the Apriori Algorithm discussed in the class supporting Transaction Reduction approach to improve the time complexity issue as a result of the repeated scans limitation of Apriori. You may compare this extended version with the earlier implementations in (1) over the same benchmark dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. \n",
    "Test drive any one implementation in (1) or (2) adopting a Vertical Transaction Database format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(data).transform(data)\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlxtend.apriori(df, min_support= 0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlxtend.fpgrowth(df, min_support= 0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. \n",
    "Using a vertical transaction database notation, generate the FI’s following the intersection approach (basic ECLAT) discussed in the class. Use earlier benchmark datasets in (1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. \n",
    "Extend the basic Apriori algorithm to generate Frequent Patterns which differentiate ab from ba (ordered patterns generation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. \n",
    "Implement following extensions to Apriori Algorithm (discussed / to be discussed in the class): Hash based strategy, Partitioning Approach and Sampling strategies. You may refer to online tutorials for a formal pseudocode description.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. \n",
    "Implement the Dynamic Itemset Counting Algorithm for Frequent Itemset Generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package py_dic:\n",
      "\n",
      "NAME\n",
      "    py_dic\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    __main__\n",
      "    dic\n",
      "    dic_tools\n",
      "    main\n",
      "\n",
      "FILE\n",
      "    /home/anant/anaconda3/lib/python3.7/site-packages/py_dic/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(py_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dic.py\n",
      "dic_tools.py\n",
      "__init__.py\n",
      "__main__.py\n",
      "main.py\n",
      "__pycache__\n",
      "settings.ini\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ls /home/anant/anaconda3/lib/python3.7/site-packages/py_dic/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. \n",
    "Test drive (download exe’s or generate one using open source versions) any three algorithm for FIM not discussed in the class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. \n",
    "For any of the real time dataset shared by my TA earlier or some other online challenge dataset, implement pre-processing strategies required and generate associations amongst the attributes of interests using any one FP implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
